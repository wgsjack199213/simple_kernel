现有TCP协议栈处理短连接的性能有一定的瓶颈，一方面内核的处理会占用大量的CPU时间片，另一方面在多核处理器上的可扩展性较差。低效性的来源是系统调用的开销，包级别的低吞吐的处理和多核系统共享资源的冲突（同步问题，局部性问题）。该项研究工作主要设计了用户态的TXP协议栈，每个核独立地并行处理TCP的请求，避开了资源共享的低效性，同时使用批量处理的思路牺牲一定程度的延迟，获得更高的吞吐量。研究者们的用户态的TCP实现相对于其他相关研究工作的优势在于提升多核可扩展性的同时，不用改写内核的代码，易于部署，同时移植性好，不用重新定义一套接口，应用程序也仅需改写少量代码（后向兼容性）。

mTCP的架构不是完全运行在用户层，Packet-level packet I/O library和网卡驱动交互，研究者们修改过的网卡驱动还是工作在内核这一层。

为了减小同步的开销，研究者们设计了per-core的无锁的数据结构，以对不同核上的流并行进行处理。

packet I/O层通过RSS（receive-side scaling）将不同的TCP连接等量地分配到不同的核上（每个核都有独立的包队列）；同时mTCP对于每一个应用线程分配一个mTCP线程，且绑定到同一个物理核上。这样的好处是保持局部性，尽量提升缓存的命中率，提升性能。

- 为什么将应用线程和mTCP线程分开？因为mTCP的操作是异步的，另外它们处于网络层次结构中的不同层次，分层设计可以简化实现。

研究者们尽量设计线程独享的数据结构，比如流哈希表，socket id manager，tcb池和socket缓存等。当不得不实现共享数据结构时，研究者们尽量避免使用锁，来减小同步开销。

- 单生产者-单消费者队列可以无锁地实现，生产者消费者分别管理维护写索引和读索引，这两个索引不存在竞争关系。

- 为了提升缓存命中率，可以尽量设计将频繁访问的变量大小控制在cache line大小的整数倍内，同时数据结构按照cache line的大小对齐。

- 另外为了减小TLB的不命中，研究者们使用了大页表。

针对短连接的一项优化是对于control信息（状态），ack包和数据包使用三个不同的队列来缓存。优先队列的设计使得在批量处理时每次优先处理重要的数据包。这样同时也一定程度上缓解了批量处理带来的延迟。

我注意到文中提到的一点局限性是研究者们开发的原型系统只支持单个应用，由于用户层的packet I/O system尚不支持多个不同的任务。研究者们称网卡虚拟化如SR-IOV等技术（为不同应用虚拟各自独有的队列）一定程度上解决这个局限。

实验评估环节主要研究处理短的TCP trasaction的性能（吞吐量，多核的可扩展性，连接accept的吞吐量），公平性和延迟（Jain's Fairness Index，各连接的transfer rate，延迟更加稳定且最坏情况得到优化），和实际应用的性能（任务的benchmark，吞吐量等）。





###Borg, Omega, and Kubernetes

*Burns B, Grant B, Oppenheimer D, et al. Borg, Omega, and Kubernetes[J]. Communications of the ACM, 2016, 59(5): 50-57.*

<https://feiskyer.github.io/assets/Borg-Omega-and-Kubernetes.pdf>

这篇文章由谷歌的工程师们所写，他们对过去十多年来谷歌中采用的几代集群容器管理系统进行了比较，对这些系统设计开发和运维中获得的经验进行了总结。

Borg对在线和离线两种业务进行调度管理，主要的设计目的在于提升集群的整体利用率（从而降低成本）。Borg的具体设计在EuroSys2015有一篇论文专门介绍。

Omega是Borg的后继产品，目的是提升Borg生态系统的软件工程（improve the software engineering啥意思?）。它应用了Borg的一些成功的设计，但是是从头开始开发的（为了获得更一致的标准化的架构）。Omega中心化地存储集群的状态，供系统不同的部件比如调度器访问，并用基于Paxos的中心化存储来管理集群中的事务transaction。Omega采用乐观并发控制（期望并发冲突只是少数情况）。这种系统组织上的解耦合有利于将Borgmaster的功能分散，不需要通过冗杂的大型中心节点进行所有的改动。

谷歌的第三代集群管理系统叫Kubernetes。谷歌将从Borg中积累的大量经验教训用在了Kubernetes上，而且将其开源了。Kubernetes和Omega类似，有中心化的共享的持久化状态存储。不同的是，Omega将状态存储直接暴露给可信的控制层组件，而Kubernetes通过domain-specific REST API（应用更高层版本，验证，语义，策略）来供更多元的客户端访问。Kubernetes的一个重要目标是让开发者更容易地部署和管理复杂分布式系统。

####容器技术

容器技术提供的资源隔离机制使得谷歌可以将集群利用率提升至显著高于业界平均水准。除了在任务级别进行混布（在线任务和离线任务）外，容器技术还可以在内核级别进行资源隔离，尽量减小进程间的互相干扰。这依靠的是谷歌在开发Borg的同时也在改进Linux container。作者们表示容器技术不是完美的，对于操作系统内核无法管理的资源，容器技术也无法做好隔离，例如内存带宽隔离，例如L3缓存隔离。（这两点Intel的下一代硬件也可以进行隔离了。Intel发展的PQOS技术即Platform Quality of Service可以实现L3缓存和内存带宽的隔离。）同时容器需要额外一层安全保护机制（例如虚拟机）来在云平台中保护容器不受到恶意程序（容器）的攻击。

现代容器技术除了资源隔离外还支持Image映像，是对容器中运行的应用的一种文件表示。

作者们强调容器的优势在于把机器导向的数据中心转换成为应用主导的数据中心。例如，容器用抽象的方式使得开发者不需要操心底层硬件和操作系统的细节，同时管理容器将管理机器转变为管理应用（API的设计），简化了应用的部署和监控。

容器抽象得以成功的关键因素在于一个独立封闭的容器映像在一个包中封装了所有的应用依赖（链接库），使得映像的可移植性大大增强。所以应用对于外部的依赖仅限于Linux内核系统调用。但目前技术并不完美，系统的许多接口仍可以和应用交互，例如/proc伪文件，ioctl的调用参数等。作者希望正在进行的容器开发研究能够更加清晰化容器抽象的表面区域。

容器在机器层面还是存在“共享库”的，机器上有base image基本映像，包含tar libc等一些东西。升级基本映像的时候偶尔会给上层应用带来一些问题。在这一点上，更现代的容器映像格式（例如Docker，ACI）除去了隐性的宿主OS依赖，用户需要要显示地用命令操作来在容器间共享映像数据。

容器提供了一些接口（HTTP端点）给管理层提供不同方面的有关应用的信息，比如应用的健康状态等等。同时容器也提供接口供管理层对应用进行设置。

容器也存在二层的嵌套架构（外层容器划分资源池，内层容器实现部署实例的隔离），在Borg中叫alloc，在Kubernetes中叫pod。

围绕容器的管理服务，谷歌建立了许多不通的系统工具，例如Chbby管理主节点竞选，BNS，监控工具，垂直/水平自动伸缩工具等。应用团队当初为了解决某些问题开发了某些工具，大家觉得好，就把它们普及开了。但一个问题是这些工具有一些怪异的API，惯例等特性，导致这些工具使得Borg生态系统部署应用越来越繁杂。这是Kubernetes想要避免的一个地方。Kubernetes的API更加标准化，采用更一致的API。

我觉得在不同的场合都会发生类似的情况，不同的团队自己造轮子，轮子造得不错就推广开来大家用。工具们会越来越繁杂难用，最后当初造轮子的人已经离职了，现在用轮子的人不懂也不敢动这些巨复杂的legacy code，就将就着继续用。直到有一天大家忍不了了，重新弄个新轮子吧。API的标准化还是很有必要的。

Kubernetes也通过对API解耦合来实现一致性。作者举了replication controller和auto-scaling system两个子系统的例子。这个例子实际上就是policy和mechanism的分离。

####教训
作者们也总结了历史中得来的一些教训，包括：

- 不要让容器系统管理端口号。于是Kubernetes给每个pod分配了各自独有的IP。

- 不要只给容器编号，给它们标签。（便于分组。）加了身份和阶段的字段。

- 小心（控制器对pod的）所有权。pods到底归谁管，标签不要冲突。

- 不要暴露原始数据。Kubernetes所有对存储的访问需要通过中心化的API服务器，它隐藏了下层实现细节。

####开放的难题
应用的配置很难管理。作者说单这一点可以用一整篇文章讨论。配置的管理难度不低于代码的管理难度。作者们觉得只能接受需求，接受像编程写配置的模式，和尽量将数据和计算分离。

依赖管理很难。
